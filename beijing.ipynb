{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "' \\n                                本市新增5例新冠肺炎确诊病例\\n\\t\\t\\t\\t\\t\\t\\t'\n",
      "http://wjw.beijing.gov.cn/xwzx_20031/wnxw/202002/t20200211_1627700.html\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "#11\n",
    "#By Yimin Zhao\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from selenium import webdriver\n",
    "\n",
    "beijing = 'http://wjw.beijing.gov.cn/xwzx_20031/wnxw/'         #卫健委的url\n",
    "info_pattern1 = re.compile(r'.*北京.*新.*冠.*疫情通报.*')  \n",
    "info_pattern2 = re.compile(r'.*新增.*例.*')\n",
    "#找到通报疫情的url\n",
    "def find_url(web_address):\n",
    "    #获取网页信息\n",
    "    while(1):\n",
    "        try:\n",
    "            headers = {\"user-agent\": \"Mizilla/5.0\"}\n",
    "            res = requests.get(web_address,timeout=(5, 10), headers = headers)\n",
    "            flag = 1\n",
    "            break\n",
    "        except:\n",
    "            print(\"retry\")\n",
    "            flag=0\n",
    "    res.encoding = 'utf-8'\n",
    "    soup = BeautifulSoup(res.text, features = 'html.parser')      #结构化处理 \n",
    "    #print(soup)\n",
    "    for element in soup.find_all(name='div',attrs = {'class':'weinei_left_con'}):\n",
    "        for info in element.find_all('a'):\n",
    "            info_title = info.get_text()      #获取通报的标题\n",
    "            print(repr(info_title))\n",
    "            if info_pattern1.match(info_title.strip()) or info_pattern2.match(info_title.strip()):\n",
    "                return info.get('href')\n",
    "                \n",
    "info_url = 'http://wjw.beijing.gov.cn/xwzx_20031/wnxw' + find_url(beijing)[1:]\n",
    "print(info_url)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "begin\n",
      "2月10日0时至24时，本市新增5例新冠肺炎确诊病例。1例有湖北及其他省份接触史；3例为确诊病例的密切接触者；1例无湖北接触史。均已送至定点医疗机构进行救治。现有疑似病例236例，新增密切接触者37例，累计确定密切接触者1502例，其中677例已解除医学观察。出院4例，死亡1例。\n",
      "        截至2月10日24时,本市累计确诊病例342例。其中出院48例，死亡3例，291例在定点医院进行隔离治疗。东城区9例、西城区39例、朝阳区56例、海淀区57例、丰台区30例、石景山区13例、门头沟区3例、房山区14例、通州区17例、顺义区10例、昌平区19例、大兴区36例、怀柔区7例、密云区6例、延庆区1例，外地来京人员25例。平谷区尚未有病例。\n",
      "finish\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "def get_data(web_address):\n",
    "    #获取网页信息\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(info_url)\n",
    "    info_content = driver.find_element_by_xpath('//*[@id=\"zoom\"]/div').text.strip()\n",
    "    print(info_content)\n",
    "    f = open(r\"D:\\IIoT\\20年寒假-疫情爬虫\\Data\\11.txt\", mode = 'w', encoding = \"utf-8\")\n",
    "    f.write(info_content)\n",
    "    f.close() \n",
    "                    \n",
    "if __name__ == '__main__':\n",
    "    print('begin')\n",
    "    get_data(info_url)\n",
    "    print('finish')\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "'Python Interactive'",
   "language": "python",
   "name": "361c5ae9-67e8-493c-b15d-1c763400a7c3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}